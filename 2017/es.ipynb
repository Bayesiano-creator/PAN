{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca28aee",
   "metadata": {},
   "source": [
    "# Initial commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac33bfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c3a628",
   "metadata": {},
   "source": [
    "# Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e87bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "_LANGUAGE_        = 'es'\n",
    "_TWEET_BATCH_SIZE_ = 3\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig()\n",
    "\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "_NO_GPUS_          = 5\n",
    "_BATCH_SIZE_       = 100\n",
    "_EPOCHS_           = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73a7a1c",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348e8e8",
   "metadata": {},
   "source": [
    "---\n",
    "Para cada lenguaje se va a utilizar un modelo diferente. En el caso de español, se utilizará Robertuito, el cual tiene su propio tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b7f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "if _LANGUAGE_ == 'es':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('pysentimiento/robertuito-base-cased')\n",
    "    \n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220061e",
   "metadata": {},
   "source": [
    "---\n",
    "Por cada lenguaje se utilizarán tres modelos diferentes. Un modelo base (Robertuito para español) y un conjunto de adapters por cada etiqueta que se quiera predecir: **gender**, **variety** y **joint**. \n",
    "\n",
    "Hay que representar las etiquetas de forma numérica: \n",
    "\n",
    "* Se utilizará siempre 0 para female y 1 para male, en el caso de **gender**. \n",
    "* Para **variety** depende de cada lenguaje. Los diccionarios asociados a cada lenguaje se crean en la siguiente celda de código. \n",
    "* Los valores numéricos de **joint** siempre estarán en función de los de gender y variety: si $ g\\in \\{ 0, 1\\} $ representa la etiqueta numérica de gender y $ v\\in \\{ 1, ..., m \\} $ la de la variety, entonces la de joint estará dada por: $$ j = g*m + v $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401e7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "gender_dict    = {'female': 0, \n",
    "                  'male':   1}\n",
    "\n",
    "varietyAR_dict = {'egypt'    : 0,\n",
    "                  'gulf'     : 1,\n",
    "                  'levantine': 2,\n",
    "                  'maghrebi' : 3}\n",
    "\n",
    "varietyEN_dict = {'australia'    : 0,\n",
    "                  'canada'       : 1,\n",
    "                  'gran britain' : 2,\n",
    "                  'ireland'      : 3,\n",
    "                  'new zeland'   : 4,\n",
    "                  'united states': 5}\n",
    "\n",
    "varietyES_dict = {'argentina': 0,\n",
    "                  'chile'    : 1,\n",
    "                  'colombia' : 2,\n",
    "                  'mexico'   : 3,\n",
    "                  'peru'     : 4,\n",
    "                  'spain'    : 5,\n",
    "                  'venezuela': 6}\n",
    "\n",
    "varietyPT_dict = {'brazil'  : 0,\n",
    "                  'portugal': 1}\n",
    "\n",
    "if _LANGUAGE_ == 'es':\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0fcd5b",
   "metadata": {},
   "source": [
    "---\n",
    "La siguiente celda contiene el bloque de código de la clase **BasePAN17**. Ésta se utilizará para cargar todos los tweets de un solo lenguaje. \n",
    "\n",
    "Ya que se utilizarán tres modelos por cada lenguaje, con diferentes etiquetas, es necesario tener tres DataLoaders. Sin embargo, estos usan los mismos tweets como entrada. La clase **BasePAN17** sirve para tener una sola instancia de los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d722b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE BASE CLASS -----------------------------------------------------------------------\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from random import shuffle\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "\n",
    "class BasePAN17():\n",
    "    \n",
    "    def __init__(self, Dir, split, language, tokenizer, gender_dict, variety_dict, tweet_batch_size):\n",
    "        self.Dir          = Dir\n",
    "        self.split        = split\n",
    "        self.language     = language\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.gender_dict  = gender_dict\n",
    "        self.variety_dict = variety_dict\n",
    "        self.tw_bsz       = tweet_batch_size\n",
    "        \n",
    "        print(\"\\nReading data...\")\n",
    "        \n",
    "        self.authors   = self.get_authors(Dir, split, language)\n",
    "        self.author_lb = self.get_author_labels(Dir, split, language)\n",
    "        self.data      = self.get_tweets_in_batches(Dir, split, language)\n",
    "        \n",
    "        shuffle(self.data)\n",
    "        \n",
    "        \n",
    "        print(\"    Done\\nPreprocessing text...\")\n",
    "        \n",
    "        preprocessed   = [preprocess_tweet(instance['text']) for instance in self.data]\n",
    "        \n",
    "        print(\"    Done\\nTokenizing...\")\n",
    "        \n",
    "        self.encodings = self.tokenizer(preprocessed, max_length = 128, \n",
    "                                                      truncation = True, \n",
    "                                                      padding    = True,\n",
    "                                                      return_token_type_ids = False)\n",
    "         \n",
    "        print(\"    Done\\n\\nTotal Instances: \" + str(len(self.data)) + '\\n')\n",
    "\n",
    "        \n",
    "    def get_authors(self, Dir, split, language):\n",
    "        path    = os.path.join(Dir, split, language)\n",
    "        files   = os.listdir(path)\n",
    "        authors = [ file[0:-4] for file in files ] \n",
    "        \n",
    "        return authors\n",
    "    \n",
    "    \n",
    "    def get_author_labels(self, Dir, split, language):\n",
    "        lb_file_name = os.path.join(Dir, split, language + '.txt')\n",
    "        lb_file      = open(lb_file_name, \"r\")\n",
    "        author_lb    = dict()\n",
    "\n",
    "        for line in lb_file:\n",
    "            author, gender, variety = line.split(':::')\n",
    "            variety = variety[:-1]                       \n",
    "\n",
    "            gl = self.gender_dict[gender]\n",
    "            vl = self.variety_dict[variety]\n",
    "            jl = gl*len(self.variety_dict) + vl\n",
    "\n",
    "            author_lb[author] = {'gender': gl, 'variety': vl, 'joint': jl}\n",
    "\n",
    "        lb_file.close()\n",
    "        \n",
    "        return author_lb\n",
    "     \n",
    "    def get_tweets_in_batches(self, Dir, split, language):\n",
    "        data   = []\n",
    "\n",
    "        for author in self.authors:\n",
    "            tw_file_name = os.path.join(Dir, split, language, author + '.xml')\n",
    "            tree         = ET.parse(tw_file_name)\n",
    "            root         = tree.getroot()\n",
    "            documents    = root[0]\n",
    "\n",
    "            for i in range(0, len(documents), self.tw_bsz):\n",
    "                doc_batch = documents[i : i + self.tw_bsz]\n",
    "                tweets    = ''\n",
    "\n",
    "                for document in doc_batch:\n",
    "                    tweets += document.text + '\\n'\n",
    "\n",
    "                data.append( {'author': author, 'text': tweets, **self.author_lb[author]} )\n",
    "        \n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184aea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "\n",
      "Total Instances: 142798\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "\n",
      "Total Instances: 95200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CREATE JUST ONE INSTANCE PER LANGUAGE ---------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "baseTrain = BasePAN17(Dir              = 'data',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_)\n",
    "\n",
    "baseTest  = BasePAN17(Dir              = 'data',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42e70f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train truncated instances:  1464\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "sizes_dict = {}\n",
    "for idx in range(len(baseTrain.data)):\n",
    "    \n",
    "    if baseTrain.encodings['input_ids'][idx][-1] != 1:\n",
    "        c += 1\n",
    "    \n",
    "    size = baseTrain.encodings['input_ids'][idx].index(2) + 1\n",
    "    if size in sizes_dict:\n",
    "        sizes_dict[size] += 1\n",
    "    else:\n",
    "        sizes_dict[size] = 1\n",
    "\n",
    "print(\"Total train truncated instances: \", c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65f5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test truncated instances:  938\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "sizes_dict = {}\n",
    "for idx in range(len(baseTest.data)):\n",
    "    \n",
    "    if baseTest.encodings['input_ids'][idx][-1] != 1:\n",
    "        c += 1\n",
    "    \n",
    "    size = baseTest.encodings['input_ids'][idx].index(2) + 1\n",
    "    if size in sizes_dict:\n",
    "        sizes_dict[size] += 1\n",
    "    else:\n",
    "        sizes_dict[size] = 1\n",
    "\n",
    "print(\"Total test truncated instances: \", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89255676",
   "metadata": {},
   "source": [
    "---\n",
    "La siguiente clase será el data loader, se usará un dataloader para cada etiqueta (gender, variety y joint) y cada splot (train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf47ce73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetPAN17(Dataset):\n",
    "    \n",
    "    def __init__(self, Base_Dataset, label):\n",
    "        self.Base_Dataset = Base_Dataset\n",
    "        self.label        = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.Base_Dataset.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.Base_Dataset.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.Base_Dataset.data[idx][self.label])\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f397a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gender  = DatasetPAN17(Base_Dataset = baseTrain, label = 'gender')\n",
    "train_variety = DatasetPAN17(Base_Dataset = baseTrain, label = 'variety')\n",
    "train_joint   = DatasetPAN17(Base_Dataset = baseTrain, label = 'joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eafcb",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a143111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/adapters/models/roberta.py:250: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/adapters/models/roberta.py:228: FutureWarning: This class has been renamed to `RobertaAdapterModel` in v3. Please use the new class instead as this class might be removed in a future version.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaModelWithHeads: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModelWithHeads\n",
    "\n",
    "if _LANGUAGE_ == 'es':\n",
    "    config = RobertaConfig.from_pretrained(\n",
    "        \"pysentimiento/robertuito-base-cased\",\n",
    "        num_labels=2,\n",
    "    )\n",
    "    model = RobertaModelWithHeads.from_pretrained(\n",
    "        \"pysentimiento/robertuito-base-cased\",\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c711fe",
   "metadata": {},
   "source": [
    "# Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1664074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModelWithHeads(\n",
       "  (shared_parameters): ModuleDict()\n",
       "  (roberta): RobertaModel(\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (invertible_adapters): ModuleDict()\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(130, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (prefix_tuning): PrefixTuningShim(\n",
       "                (pool): PrefixTuningPool(\n",
       "                  (prefix_tunings): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (adapters): ModuleDict(\n",
       "              (gender): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (variety): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "              (joint): ParallelAdapter(\n",
       "                (non_linearity): Activation_Function_Class(\n",
       "                  (f): ReLU()\n",
       "                )\n",
       "                (adapter_down): Sequential(\n",
       "                  (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "                  (1): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (adapter_fusion_layer): ModuleDict()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (gender): ClassificationHead(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Activation_Function_Class(\n",
       "        (f): Tanh()\n",
       "      )\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=768, out_features=2, bias=True)\n",
       "    )\n",
       "    (variety): ClassificationHead(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Activation_Function_Class(\n",
       "        (f): Tanh()\n",
       "      )\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=768, out_features=7, bias=True)\n",
       "    )\n",
       "    (joint): ClassificationHead(\n",
       "      (0): Dropout(p=0.1, inplace=False)\n",
       "      (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (2): Activation_Function_Class(\n",
       "        (f): Tanh()\n",
       "      )\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=768, out_features=14, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the task\n",
    "\n",
    "tasks           = [\"gender\", \"variety\", \"joint\"]\n",
    "num_v           = len(baseTrain.variety_dict)\n",
    "num_labels_dict = {\"gender\": 2, \"variety\": num_v, \"joint\": 2*num_v}\n",
    "\n",
    "\n",
    "# Add adapters\n",
    "\n",
    "for task_name in tasks:\n",
    "    \n",
    "    model.add_adapter(\n",
    "        adapter_name = task_name, \n",
    "        config       = _ADAPTER_CONFIG_\n",
    "    )\n",
    "    \n",
    "    model.add_classification_head(\n",
    "        head_name    = task_name,\n",
    "        num_labels   = num_labels_dict[task_name],\n",
    "      )\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da0419",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4caaa65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AdapterTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a59601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 142798\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 100\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 500\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2860\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 34:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output_gender/checkpoint-500\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_gender/checkpoint-1000\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_gender/checkpoint-1500\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_gender/checkpoint-2000\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_gender/checkpoint-2500\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/gender/adapter_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in ./training_output_gender/checkpoint-2500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_gender/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_gender/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 142798\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 100\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 500\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 35:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output_variety/checkpoint-500\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_variety/checkpoint-1000\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_variety/checkpoint-1500\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_variety/checkpoint-2000\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_variety/checkpoint-2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./training_output_variety/checkpoint-2500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_variety/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_variety/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 142798\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 100\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 500\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 34:30, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./training_output_joint/checkpoint-500\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_joint/checkpoint-1000\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1000/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_joint/checkpoint-1500\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-1500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-1500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_joint/checkpoint-2000\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/gender/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2000/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2000/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to ./training_output_joint/checkpoint-2500\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/gender/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/gender/pytorch_adapter.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./training_output_joint/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/variety/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/variety/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/joint/adapter_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/joint/pytorch_adapter.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/variety/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/variety/pytorch_model_head.bin\n",
      "Configuration saved in ./training_output_joint/checkpoint-2500/joint/head_config.json\n",
      "Module weights saved in ./training_output_joint/checkpoint-2500/joint/pytorch_model_head.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_dict = {\"gender\": train_gender, \"variety\": train_variety, \"joint\": train_joint}\n",
    "test_dict    = {\"gender\": train_gender, \"variety\": train_variety, \"joint\": train_joint}\n",
    "\n",
    "for task_name in tasks:\n",
    "    \n",
    "    model.set_active_adapters(task_name)\n",
    "    model.train_adapter(task_name)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        learning_rate               = 1e-4,\n",
    "        #weight_decay                 = 0.01,\n",
    "        num_train_epochs            = _EPOCHS_,\n",
    "        per_device_train_batch_size = _BATCH_SIZE_,\n",
    "        per_device_eval_batch_size  = _BATCH_SIZE_,\n",
    "        logging_steps               = (len(baseTrain.data)/(_BATCH_SIZE_*_NO_GPUS_))/5 ,\n",
    "        output_dir                  = \"./training_output_\" + task_name,\n",
    "        overwrite_output_dir        = True,\n",
    "        # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "        remove_unused_columns       = False,\n",
    "    )\n",
    "\n",
    "    trainer = AdapterTrainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = dataset_dict[task_name],\n",
    "    )\n",
    "    trainer.args._n_gpu = _NO_GPUS_\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5088bd7e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "082eba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers.adapters.composition as AC  \n",
    "\n",
    "model.set_active_adapters(AC.Parallel(*tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4938ca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2800/2800 [25:08<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "successful_preds = { task: 0 for task in tasks }\n",
    "\n",
    "with torch.no_grad():\n",
    "    for author in tqdm(baseTest.authors):\n",
    "        # finds all instances of author\n",
    "        author_idx = [idx for idx in range(len(baseTest.data)) if baseTest.data[idx]['author'] == author]\n",
    "        \n",
    "        # get truth labels with fst instance and initialize scores\n",
    "        fst      = baseTest.data[author_idx[0]]\n",
    "        truth    = { task: fst[task]                         for task in tasks }\n",
    "        scores   = { task: np.zeros( num_labels_dict[task] ) for task in tasks }\n",
    "        \n",
    "        for idx in author_idx:\n",
    "            # creates case in device\n",
    "            case = {key: torch.tensor(val[idx]).to(device) for key, val in baseTest.encodings.items()}\n",
    "\n",
    "            # computes all task predictions in parallel\n",
    "            preds = list( model(**case) )\n",
    "            \n",
    "            # get prediction and accumulate\n",
    "            for task, pred in zip(tasks, preds):\n",
    "                y = torch.nn.functional.softmax(pred['logits'], dim = 1).cpu().numpy()[0]\n",
    "                #print(task, y)\n",
    "                scores[task] += y\n",
    "        \n",
    "        for task in tasks:\n",
    "            if np.argmax( scores[task] ) == truth[task]:\n",
    "                successful_preds[task] += 1\n",
    "\n",
    "accuracy = { task: val/len(baseTest.authors) for task, val in successful_preds.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfe0dc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the three tasks\n",
      "{'gender': 0.8232142857142857, 'variety': 0.9439285714285715, 'joint': 0.7667857142857143}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in the three tasks\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31708b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the three tasks\n",
      "{'gender': 0.8442857142857143, 'variety': 0.95, 'joint': 0.7960714285714285}\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in the three tasks\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0471370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy\n",
      "{'gender': 0.8321, 'variety': 0.9625, 'joint': 0.8036}\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = {\"gender\": 0.8321, \"variety\": 0.9625, \"joint\": 0.8036}\n",
    "print(\"Best accuracy\")\n",
    "print(best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
