{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffd56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a7c20eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "_LANGUAGE_         = 'es'\n",
    "_PRETRAINED_LM_    = 'pysentimiento/robertuito-base-cased'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(mh_adapter = True, reduction_factor = 32)\n",
    "_MAX_SEQ_LEN_      = 128\n",
    "_OUTPUT_DIR_       = 'adapter_checkPoints'\n",
    "_LOGGING_STEPS_    = 2\n",
    "_NUM_AUTHORS_      = [8]\n",
    "_K_FOLD_CV_        = 5\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "_NO_GPUS_          = 2\n",
    "_BATCH_SIZE_       = int(64 / _NO_GPUS_)\n",
    "_EPOCHS_           = {'gender': 20, 'variety': 20}\n",
    "_LEARNING_RATE_    = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24462750",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13393cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "gender_dict    = {'female': 0, \n",
    "                  'male':   1}\n",
    "\n",
    "varietyEN_dict = {'australia'    : 0,\n",
    "                  'canada'       : 1,\n",
    "                  'great britain' : 2,\n",
    "                  'ireland'      : 3,\n",
    "                  'new zealand'   : 4,\n",
    "                  'united states': 5}\n",
    "\n",
    "varietyES_dict = {'argentina': 0,\n",
    "                  'chile'    : 1,\n",
    "                  'colombia' : 2,\n",
    "                  'mexico'   : 3,\n",
    "                  'peru'     : 4,\n",
    "                  'spain'    : 5,\n",
    "                  'venezuela': 6}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a6fa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DIRECTORY\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    variety_dict = varietyEN_dict\n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ede52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509445d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d36aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetPAN17 import BasePAN17, DatasetPAN17, DatasetCrossVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef668d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 84000\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 56000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseTrain  = BasePAN17(Dir             = '../data',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)\n",
    "\n",
    "baseTest  = BasePAN17(Dir             = '../data',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "143f62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = DatasetPAN17(Base_Dataset = baseTest, label = 'gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bba9d21",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e07a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e520320",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 8 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 45/100 03:56 < 05:02, 0.18 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.676100</td>\n",
       "      <td>0.699052</td>\n",
       "      <td>0.507619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.622300</td>\n",
       "      <td>0.697157</td>\n",
       "      <td>0.526071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.696875</td>\n",
       "      <td>0.536012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.529000</td>\n",
       "      <td>0.700067</td>\n",
       "      <td>0.541012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.482100</td>\n",
       "      <td>0.707808</td>\n",
       "      <td>0.540893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.428600</td>\n",
       "      <td>0.718779</td>\n",
       "      <td>0.543214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.731943</td>\n",
       "      <td>0.544107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.348300</td>\n",
       "      <td>0.747850</td>\n",
       "      <td>0.544167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.309000</td>\n",
       "      <td>0.765231</td>\n",
       "      <td>0.541845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-5\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-5/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-15\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-15/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-25\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-25/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-5] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-35\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-35/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-15] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-45\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-45/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-45/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-40 (score: 0.5441666666666667).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.57: 100%|███████████████████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 142.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='70' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 70/100 06:05 < 02:41, 0.19 it/s, Epoch 14/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.712900</td>\n",
       "      <td>0.686068</td>\n",
       "      <td>0.546726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.678914</td>\n",
       "      <td>0.572679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610700</td>\n",
       "      <td>0.673023</td>\n",
       "      <td>0.586607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.590600</td>\n",
       "      <td>0.668605</td>\n",
       "      <td>0.595714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.554000</td>\n",
       "      <td>0.665147</td>\n",
       "      <td>0.601071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.662452</td>\n",
       "      <td>0.606429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.660634</td>\n",
       "      <td>0.609881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.660187</td>\n",
       "      <td>0.611190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.438600</td>\n",
       "      <td>0.661424</td>\n",
       "      <td>0.610714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>0.660498</td>\n",
       "      <td>0.613214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.661428</td>\n",
       "      <td>0.613929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.366200</td>\n",
       "      <td>0.662766</td>\n",
       "      <td>0.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.664819</td>\n",
       "      <td>0.612917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.667403</td>\n",
       "      <td>0.611786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-5\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-5/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-25] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-15\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-15/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-35] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-25\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-25/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-45] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-5] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-35\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-35/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-15] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-45\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-45/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-45/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-25] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-55\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-55/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-55/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-35] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-65\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-65/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-65/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-45] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-60 (score: 0.6142857142857143).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6742857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 141.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90/100 07:54 < 00:53, 0.19 it/s, Epoch 18/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>0.682440</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.671467</td>\n",
       "      <td>0.595119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.600600</td>\n",
       "      <td>0.662092</td>\n",
       "      <td>0.615357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.655118</td>\n",
       "      <td>0.624762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.649915</td>\n",
       "      <td>0.631488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.645817</td>\n",
       "      <td>0.636548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.641993</td>\n",
       "      <td>0.640536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.639628</td>\n",
       "      <td>0.642738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.639295</td>\n",
       "      <td>0.641488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.638436</td>\n",
       "      <td>0.643036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.346500</td>\n",
       "      <td>0.639202</td>\n",
       "      <td>0.643929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.332300</td>\n",
       "      <td>0.640688</td>\n",
       "      <td>0.644464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.318900</td>\n",
       "      <td>0.642204</td>\n",
       "      <td>0.644524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.642999</td>\n",
       "      <td>0.645357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.285600</td>\n",
       "      <td>0.642609</td>\n",
       "      <td>0.646964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.268400</td>\n",
       "      <td>0.643131</td>\n",
       "      <td>0.647143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.265300</td>\n",
       "      <td>0.644906</td>\n",
       "      <td>0.646667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.267200</td>\n",
       "      <td>0.646228</td>\n",
       "      <td>0.645595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-5\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-5/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-55] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-15\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-15/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-65] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-25\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-25/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-5] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-35\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-35/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-15] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-45\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-45/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-45/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-25] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-55\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-55/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-55/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-35] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-65\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-65/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-65/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-45] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-75\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-75/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-75/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-55] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-85\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-85/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-85/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-65] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6471428571428571).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6610714285714285: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:20<00:00, 139.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 30/100 02:36 < 06:32, 0.18 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669000</td>\n",
       "      <td>0.674633</td>\n",
       "      <td>0.585179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.608200</td>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.610476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.574200</td>\n",
       "      <td>0.659438</td>\n",
       "      <td>0.619464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>0.657033</td>\n",
       "      <td>0.620714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.657665</td>\n",
       "      <td>0.618393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.467000</td>\n",
       "      <td>0.657760</td>\n",
       "      <td>0.620357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-5\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-5/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-75] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-15\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-15/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-85] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-25\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-25/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-5] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-20 (score: 0.6207142857142857).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.655: 100%|██████████████████████████████████████████████████████████████████| 2800/2800 [00:20<00:00, 139.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65/100 05:39 < 03:08, 0.19 it/s, Epoch 13/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.653200</td>\n",
       "      <td>0.668472</td>\n",
       "      <td>0.601667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.611100</td>\n",
       "      <td>0.658887</td>\n",
       "      <td>0.617619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.650002</td>\n",
       "      <td>0.630179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.518600</td>\n",
       "      <td>0.643739</td>\n",
       "      <td>0.638869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.485100</td>\n",
       "      <td>0.640460</td>\n",
       "      <td>0.640536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.638994</td>\n",
       "      <td>0.640357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.414200</td>\n",
       "      <td>0.637970</td>\n",
       "      <td>0.642440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.638460</td>\n",
       "      <td>0.642738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.640098</td>\n",
       "      <td>0.644464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.337200</td>\n",
       "      <td>0.639729</td>\n",
       "      <td>0.645893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.642041</td>\n",
       "      <td>0.646905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.645449</td>\n",
       "      <td>0.645536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.281900</td>\n",
       "      <td>0.649768</td>\n",
       "      <td>0.645714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-5\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-5/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-5/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-15] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-15\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-15/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-15/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-25] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-25\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-25/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-25/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-5] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-35\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-35/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-35/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-15] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-45\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-45/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-45/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-25] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-55\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-55/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-55/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-35] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-65\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-65/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-65/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-55 (score: 0.6469047619047619).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6714285714285714: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:20<00:00, 138.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 8 authors per label:  {'accuracy': [0.6463571428571429, 0.038807741706089145], 'f1-score': [0.675614778161259, 0.05229066275650628]}\n",
      "Working with 16 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 50/200 02:15 < 07:04, 0.35 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.683410</td>\n",
       "      <td>0.552679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.666250</td>\n",
       "      <td>0.587440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.563900</td>\n",
       "      <td>0.657527</td>\n",
       "      <td>0.610714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.662672</td>\n",
       "      <td>0.606726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.470800</td>\n",
       "      <td>0.672755</td>\n",
       "      <td>0.606786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-45] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-55] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-65] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-30 (score: 0.6107142857142858).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6392857142857142: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:20<00:00, 134.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 80/200 03:47 < 05:50, 0.34 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.700400</td>\n",
       "      <td>0.681026</td>\n",
       "      <td>0.553095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.613600</td>\n",
       "      <td>0.662550</td>\n",
       "      <td>0.596310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.549500</td>\n",
       "      <td>0.654685</td>\n",
       "      <td>0.613690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.653301</td>\n",
       "      <td>0.613274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445600</td>\n",
       "      <td>0.655705</td>\n",
       "      <td>0.616964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.664231</td>\n",
       "      <td>0.621369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.672209</td>\n",
       "      <td>0.618810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.686081</td>\n",
       "      <td>0.618571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-60 (score: 0.6213690476190477).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6785714285714286: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:21<00:00, 131.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/200 04:35 < 04:41, 0.36 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677700</td>\n",
       "      <td>0.676555</td>\n",
       "      <td>0.572560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.652451</td>\n",
       "      <td>0.621786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.551300</td>\n",
       "      <td>0.638769</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>0.632060</td>\n",
       "      <td>0.642202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.629266</td>\n",
       "      <td>0.645476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.401700</td>\n",
       "      <td>0.636041</td>\n",
       "      <td>0.644405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.638745</td>\n",
       "      <td>0.647024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.648327</td>\n",
       "      <td>0.648214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.658870</td>\n",
       "      <td>0.647381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.255200</td>\n",
       "      <td>0.669372</td>\n",
       "      <td>0.646488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6482142857142857).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6810714285714285: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 147.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/200 05:01 < 04:11, 0.36 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703300</td>\n",
       "      <td>0.682389</td>\n",
       "      <td>0.567083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.624100</td>\n",
       "      <td>0.662165</td>\n",
       "      <td>0.605714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565400</td>\n",
       "      <td>0.656514</td>\n",
       "      <td>0.606905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.516600</td>\n",
       "      <td>0.651550</td>\n",
       "      <td>0.616250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.650754</td>\n",
       "      <td>0.625417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.414100</td>\n",
       "      <td>0.659619</td>\n",
       "      <td>0.622619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.664133</td>\n",
       "      <td>0.628631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.678446</td>\n",
       "      <td>0.627083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.685040</td>\n",
       "      <td>0.634286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.700194</td>\n",
       "      <td>0.633095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.715184</td>\n",
       "      <td>0.629345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-100] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-110\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-110/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-110/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-90 (score: 0.6342857142857142).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6853571428571429: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 147.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90/200 04:06 < 05:07, 0.36 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677800</td>\n",
       "      <td>0.676200</td>\n",
       "      <td>0.582976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.657082</td>\n",
       "      <td>0.615238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.560300</td>\n",
       "      <td>0.644828</td>\n",
       "      <td>0.629286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.635279</td>\n",
       "      <td>0.639345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.451400</td>\n",
       "      <td>0.631386</td>\n",
       "      <td>0.645179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.403200</td>\n",
       "      <td>0.636126</td>\n",
       "      <td>0.643333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.638243</td>\n",
       "      <td>0.647381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.319600</td>\n",
       "      <td>0.647030</td>\n",
       "      <td>0.645655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.289800</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.644464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-10\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-10/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-10/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-100] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-50\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-50/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-50/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-110] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-10] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-70\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-70/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-70/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-70 (score: 0.6473809523809524).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6682142857142858: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 152.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 16 authors per label:  {'accuracy': [0.6705, 0.01659634825828275], 'f1-score': [0.6892745171156573, 0.02207377870412438]}\n",
      "Working with 32 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 80/400 02:00 < 08:12, 0.65 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.687700</td>\n",
       "      <td>0.673765</td>\n",
       "      <td>0.573810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.659639</td>\n",
       "      <td>0.594643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.666789</td>\n",
       "      <td>0.589702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.680066</td>\n",
       "      <td>0.591250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-50] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-70] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-40 (score: 0.5946428571428571).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6621428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 147.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/400 03:05 < 07:20, 0.64 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.668700</td>\n",
       "      <td>0.659408</td>\n",
       "      <td>0.600238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.587500</td>\n",
       "      <td>0.651759</td>\n",
       "      <td>0.616488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.655559</td>\n",
       "      <td>0.623810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.458500</td>\n",
       "      <td>0.666596</td>\n",
       "      <td>0.629107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.704322</td>\n",
       "      <td>0.626369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.341000</td>\n",
       "      <td>0.732233</td>\n",
       "      <td>0.628274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6291071428571429).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6807142857142857: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 153.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/400 03:08 < 07:26, 0.63 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.648700</td>\n",
       "      <td>0.633959</td>\n",
       "      <td>0.655714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.623559</td>\n",
       "      <td>0.665119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.623642</td>\n",
       "      <td>0.665238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.619589</td>\n",
       "      <td>0.673155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.659452</td>\n",
       "      <td>0.658750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.679911</td>\n",
       "      <td>0.657202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6731547619047619).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6842857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 146.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/400 03:01 < 07:10, 0.65 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.648700</td>\n",
       "      <td>0.650831</td>\n",
       "      <td>0.617857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.647267</td>\n",
       "      <td>0.632857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.651502</td>\n",
       "      <td>0.636964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.650399</td>\n",
       "      <td>0.641369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.694828</td>\n",
       "      <td>0.635952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.719544</td>\n",
       "      <td>0.635357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-100] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6413690476190477).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6842857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 145.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/400 03:07 < 07:24, 0.63 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.648700</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>0.644643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.631308</td>\n",
       "      <td>0.654345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.634190</td>\n",
       "      <td>0.656071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.631200</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.381600</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.651786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.325700</td>\n",
       "      <td>0.702042</td>\n",
       "      <td>0.647738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-20\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-20/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-20/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-100\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-100/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-100/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6607142857142857).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6842857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 148.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 32 authors per label:  {'accuracy': [0.679142857142857, 0.008611809641772302], 'f1-score': [0.6896027128017307, 0.006236413401855358]}\n",
      "Working with 48 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 90/600 01:40 < 09:40, 0.88 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669400</td>\n",
       "      <td>0.653440</td>\n",
       "      <td>0.608750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.566400</td>\n",
       "      <td>0.671338</td>\n",
       "      <td>0.593929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.694248</td>\n",
       "      <td>0.596667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-100] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-30 (score: 0.60875).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6542857142857142: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 144.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/600 04:27 < 06:44, 0.89 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649900</td>\n",
       "      <td>0.660337</td>\n",
       "      <td>0.607321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.660121</td>\n",
       "      <td>0.620536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.491500</td>\n",
       "      <td>0.681453</td>\n",
       "      <td>0.616548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.707673</td>\n",
       "      <td>0.621726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>0.723056</td>\n",
       "      <td>0.628929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.317300</td>\n",
       "      <td>0.752430</td>\n",
       "      <td>0.633869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.789026</td>\n",
       "      <td>0.631905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.850971</td>\n",
       "      <td>0.629167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-20] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-150\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-150/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-180\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-180/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-180/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-210\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-210/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-210/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-180 (score: 0.6338690476190476).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6882142857142857: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:18<00:00, 147.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/600 02:10 < 08:52, 0.90 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.633269</td>\n",
       "      <td>0.654821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.613434</td>\n",
       "      <td>0.670714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.628318</td>\n",
       "      <td>0.664048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>0.648896</td>\n",
       "      <td>0.663690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-150] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-180] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-60 (score: 0.6707142857142857).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.68: 100%|███████████████████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 144.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/600 04:34 < 06:55, 0.87 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.653040</td>\n",
       "      <td>0.631071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.645706</td>\n",
       "      <td>0.636429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.478700</td>\n",
       "      <td>0.668099</td>\n",
       "      <td>0.634583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.693787</td>\n",
       "      <td>0.637024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.726987</td>\n",
       "      <td>0.636667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.294900</td>\n",
       "      <td>0.762566</td>\n",
       "      <td>0.640298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.242600</td>\n",
       "      <td>0.812519</td>\n",
       "      <td>0.637381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.196900</td>\n",
       "      <td>0.883757</td>\n",
       "      <td>0.637262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-150\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-150/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-150/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-180\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-180/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-180/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-210\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-210/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-210/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-180 (score: 0.6402976190476191).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6946428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 146.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1920\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/600 02:11 < 08:56, 0.89 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>0.633858</td>\n",
       "      <td>0.660060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.559700</td>\n",
       "      <td>0.616724</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.483900</td>\n",
       "      <td>0.636408</td>\n",
       "      <td>0.668214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.410900</td>\n",
       "      <td>0.662292</td>\n",
       "      <td>0.666131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-30\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-30/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-30/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-60\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-60/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-60/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-150] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-90\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-90/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-90/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-180] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-210] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-60 (score: 0.675).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.68: 100%|███████████████████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 145.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 48 authors per label:  {'accuracy': [0.6794285714285715, 0.013722095990312377], 'f1-score': [0.6935243485276106, 0.025162403949095986]}\n",
      "Working with 64 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/800 02:23 < 09:40, 1.10 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.657800</td>\n",
       "      <td>0.656426</td>\n",
       "      <td>0.596190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.670281</td>\n",
       "      <td>0.601190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.480100</td>\n",
       "      <td>0.722978</td>\n",
       "      <td>0.597440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.762094</td>\n",
       "      <td>0.599940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-30] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-60] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-80 (score: 0.6011904761904762).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6225: 100%|█████████████████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 142.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/800 05:23 < 06:37, 1.11 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>0.643228</td>\n",
       "      <td>0.622679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.565200</td>\n",
       "      <td>0.639114</td>\n",
       "      <td>0.636964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.488300</td>\n",
       "      <td>0.655250</td>\n",
       "      <td>0.639286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.697719</td>\n",
       "      <td>0.638452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.639821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.294700</td>\n",
       "      <td>0.779493</td>\n",
       "      <td>0.642143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.642560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.190800</td>\n",
       "      <td>0.899205</td>\n",
       "      <td>0.641786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.977406</td>\n",
       "      <td>0.640893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-200\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-200/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-90] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-360\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-360/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-360/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-280 (score: 0.6425595238095239).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7089285714285715: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 141.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/800 02:56 < 08:53, 1.12 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.618105</td>\n",
       "      <td>0.670774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.599580</td>\n",
       "      <td>0.679881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.500300</td>\n",
       "      <td>0.602091</td>\n",
       "      <td>0.680476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.422100</td>\n",
       "      <td>0.624915</td>\n",
       "      <td>0.674464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.665167</td>\n",
       "      <td>0.674524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-200] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-280] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-200\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-200/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-200/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-360] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-120 (score: 0.6804761904761905).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7107142857142857: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 143.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/800 03:04 < 09:19, 1.07 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.649370</td>\n",
       "      <td>0.630952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.640664</td>\n",
       "      <td>0.636786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.652628</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>0.634583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>0.730098</td>\n",
       "      <td>0.636905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-200\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-200/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-200/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-120 (score: 0.64).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6996428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 145.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2560\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/800 03:04 < 09:19, 1.07 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.655800</td>\n",
       "      <td>0.638516</td>\n",
       "      <td>0.645476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.570500</td>\n",
       "      <td>0.625606</td>\n",
       "      <td>0.655476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.632415</td>\n",
       "      <td>0.659702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.661333</td>\n",
       "      <td>0.651429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.352900</td>\n",
       "      <td>0.702288</td>\n",
       "      <td>0.652738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-40\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-40/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-40/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-120/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-200\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-200/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-200/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-120 (score: 0.659702380952381).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.6996428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 144.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 64 authors per label:  {'accuracy': [0.6882857142857143, 0.03321113656345916], 'f1-score': [0.6787820866714995, 0.048350822584644085]}\n",
      "Working with 128 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 560/1600 05:21 < 09:58, 1.74 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.639300</td>\n",
       "      <td>0.630892</td>\n",
       "      <td>0.633452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.550100</td>\n",
       "      <td>0.635357</td>\n",
       "      <td>0.640714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.648297</td>\n",
       "      <td>0.646786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.683748</td>\n",
       "      <td>0.647857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.733024</td>\n",
       "      <td>0.648690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.260700</td>\n",
       "      <td>0.790741</td>\n",
       "      <td>0.643869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.865234</td>\n",
       "      <td>0.635298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-40] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-400\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-400/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-560\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-560/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-560/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-200] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-400 (score: 0.6486904761904762).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7164285714285714: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 146.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 480/1600 04:31 < 10:36, 1.76 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.631400</td>\n",
       "      <td>0.647354</td>\n",
       "      <td>0.630238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.535500</td>\n",
       "      <td>0.639391</td>\n",
       "      <td>0.648869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.658887</td>\n",
       "      <td>0.654583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.682248</td>\n",
       "      <td>0.659226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.315400</td>\n",
       "      <td>0.738015</td>\n",
       "      <td>0.658631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.253200</td>\n",
       "      <td>0.798669</td>\n",
       "      <td>0.658810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-400] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-400\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-400/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-560] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-320 (score: 0.6592261904761905).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7360714285714286: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 145.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/1600 03:46 < 11:22, 1.76 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.612700</td>\n",
       "      <td>0.600431</td>\n",
       "      <td>0.685060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.596502</td>\n",
       "      <td>0.687202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.688810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>0.648070</td>\n",
       "      <td>0.688571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.690581</td>\n",
       "      <td>0.683750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-400] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-400\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-400/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-400/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-240 (score: 0.6888095238095238).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7367857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 146.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 400/1600 03:54 < 11:46, 1.70 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.639465</td>\n",
       "      <td>0.647679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.630644</td>\n",
       "      <td>0.663929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.657460</td>\n",
       "      <td>0.664821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.683470</td>\n",
       "      <td>0.663512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.746672</td>\n",
       "      <td>0.662857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-400\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-400/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-240 (score: 0.6648214285714286).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7271428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 145.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 560/1600 05:26 < 10:08, 1.71 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.620442</td>\n",
       "      <td>0.661726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.605438</td>\n",
       "      <td>0.674345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.624865</td>\n",
       "      <td>0.674464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.648827</td>\n",
       "      <td>0.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.311100</td>\n",
       "      <td>0.701599</td>\n",
       "      <td>0.678155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.239100</td>\n",
       "      <td>0.764512</td>\n",
       "      <td>0.676786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.821654</td>\n",
       "      <td>0.677143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-80\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-80/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-80/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-240/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-400\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-400/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-400/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-80] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-560\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-560/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-560/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-400 (score: 0.6781547619047619).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7296428571428571: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:19<00:00, 144.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 128 authors per label:  {'accuracy': [0.7292142857142858, 0.007379646581881108], 'f1-score': [0.7342845402829002, 0.013718943601915282]}\n",
      "Working with 256 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10240\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1440' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1440/3200 10:00 < 12:14, 2.40 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>0.608448</td>\n",
       "      <td>0.661726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.532100</td>\n",
       "      <td>0.596824</td>\n",
       "      <td>0.679702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.446500</td>\n",
       "      <td>0.618030</td>\n",
       "      <td>0.679940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.361900</td>\n",
       "      <td>0.654262</td>\n",
       "      <td>0.681964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.702700</td>\n",
       "      <td>0.684107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.216900</td>\n",
       "      <td>0.785139</td>\n",
       "      <td>0.682798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.908229</td>\n",
       "      <td>0.687024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>1.020597</td>\n",
       "      <td>0.672440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>1.156024</td>\n",
       "      <td>0.678929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-800\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-800/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-400] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-560] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1440\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1440/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1440/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-640] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-1120 (score: 0.6870238095238095).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.8021428571428572: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 160.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10240\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1280' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1280/3200 08:41 < 13:03, 2.45 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.613300</td>\n",
       "      <td>0.604408</td>\n",
       "      <td>0.666607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.606181</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>0.635277</td>\n",
       "      <td>0.684345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.685791</td>\n",
       "      <td>0.689167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.275400</td>\n",
       "      <td>0.739154</td>\n",
       "      <td>0.686786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.810942</td>\n",
       "      <td>0.691012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.164800</td>\n",
       "      <td>0.906659</td>\n",
       "      <td>0.687083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.993968</td>\n",
       "      <td>0.686488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-800] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-960] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1280] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-800\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-800/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1440] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1120\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1120/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1120/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-960 (score: 0.6910119047619048).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7867857142857143: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:16<00:00, 164.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10240\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 960/3200 06:31 < 15:14, 2.45 it/s, Epoch 6/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>0.580996</td>\n",
       "      <td>0.688929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.505400</td>\n",
       "      <td>0.576294</td>\n",
       "      <td>0.699107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.424000</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.699345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.646093</td>\n",
       "      <td>0.701964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.716445</td>\n",
       "      <td>0.695357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.198700</td>\n",
       "      <td>0.797181</td>\n",
       "      <td>0.699821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-640] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-800] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-960] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1120] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-800\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-800/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1280] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-640 (score: 0.7019642857142857).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7696428571428572: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 156.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10240\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/3200 05:26 < 16:23, 2.44 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.589458</td>\n",
       "      <td>0.678750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.500800</td>\n",
       "      <td>0.592655</td>\n",
       "      <td>0.687202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.617783</td>\n",
       "      <td>0.695000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.675530</td>\n",
       "      <td>0.692262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.738575</td>\n",
       "      <td>0.692917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-640] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-800] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-800\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-800/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-800/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-960] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-480 (score: 0.695).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.775: 100%|██████████████████████████████████████████████████████████████████| 2800/2800 [00:16<00:00, 166.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 10240\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3200\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='3200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/3200 05:36 < 16:50, 2.37 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.611200</td>\n",
       "      <td>0.575416</td>\n",
       "      <td>0.690893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>0.561788</td>\n",
       "      <td>0.707262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.428000</td>\n",
       "      <td>0.578867</td>\n",
       "      <td>0.708690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.342700</td>\n",
       "      <td>0.615145</td>\n",
       "      <td>0.708274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.674154</td>\n",
       "      <td>0.703512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-160\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-160/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-160/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-480\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-480/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-480/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-800\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-800/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-800/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-480 (score: 0.7086904761904762).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7703571428571429: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 162.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 256 authors per label:  {'accuracy': [0.7807857142857142, 0.012316821084705786], 'f1-score': [0.775970202717284, 0.014919221734025953]}\n",
      "Working with 512 authors per label ... \n",
      "Train,Val split number 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/6400 08:57 < 26:55, 2.97 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.596000</td>\n",
       "      <td>0.590273</td>\n",
       "      <td>0.685060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.579206</td>\n",
       "      <td>0.700298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.406400</td>\n",
       "      <td>0.613870</td>\n",
       "      <td>0.707024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.686476</td>\n",
       "      <td>0.700655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.251300</td>\n",
       "      <td>0.763468</td>\n",
       "      <td>0.702381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-160] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1600\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1600/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-480] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-960 (score: 0.7070238095238095).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.8207142857142857: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 161.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 2 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2560' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2560/6400 14:20 < 21:31, 2.97 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.599927</td>\n",
       "      <td>0.680536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.489600</td>\n",
       "      <td>0.597196</td>\n",
       "      <td>0.696726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.665928</td>\n",
       "      <td>0.693452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>0.693193</td>\n",
       "      <td>0.698214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.776772</td>\n",
       "      <td>0.697083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>0.868872</td>\n",
       "      <td>0.699226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.994927</td>\n",
       "      <td>0.694405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.097100</td>\n",
       "      <td>1.179136</td>\n",
       "      <td>0.690417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-640] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-800] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1600\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1600/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1600/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1920\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1920/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1920/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-960] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-2240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-2240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-2240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1280] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-2560\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-2560/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-2560/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1600] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-1920 (score: 0.6992261904761905).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.8114285714285714: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 159.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 3 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2240' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2240/6400 12:32 < 23:18, 2.98 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595400</td>\n",
       "      <td>0.574063</td>\n",
       "      <td>0.693333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.591658</td>\n",
       "      <td>0.692381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.621338</td>\n",
       "      <td>0.697024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.679599</td>\n",
       "      <td>0.686786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.247700</td>\n",
       "      <td>0.739347</td>\n",
       "      <td>0.701429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>0.889384</td>\n",
       "      <td>0.682976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.985050</td>\n",
       "      <td>0.696190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-320] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-640] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1600\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1600/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1920] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1920\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1920/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1920/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-2240] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-2240\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-2240/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-2240/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-2560] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-1600 (score: 0.7014285714285714).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.8153571428571429: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 160.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 4 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/6400 08:52 < 26:40, 3.00 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.600300</td>\n",
       "      <td>0.588046</td>\n",
       "      <td>0.682560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.606565</td>\n",
       "      <td>0.688155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.403400</td>\n",
       "      <td>0.643012</td>\n",
       "      <td>0.691429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.323600</td>\n",
       "      <td>0.704050</td>\n",
       "      <td>0.680774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.787207</td>\n",
       "      <td>0.687262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-960] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1280] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1600] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-1920] due to args.save_total_limit\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1600\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1600/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1600/pytorch_model.bin\n",
      "Deleting older checkpoint [adapter_checkPoints/gender/checkpoint-2240] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-960 (score: 0.6914285714285714).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7989285714285714: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 162.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train,Val split number 5 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/3f85c0ee804baf604258892a88dd52cdf051d2418a511dcab7cab99a85a3a1b3.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-cased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/795f97c54d814fec7e7c661c939f5f797bd6fb98c93716c51ca7f06335899b9f.27f4ebde81f46ec68cbdd9518932c83dd3d3eac62e312dedfb680d87341e94e9\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-cased were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-cased and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6400\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='6400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/6400 09:02 < 27:09, 2.95 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.598900</td>\n",
       "      <td>0.560749</td>\n",
       "      <td>0.706250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.578273</td>\n",
       "      <td>0.706964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.405400</td>\n",
       "      <td>0.598350</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.654239</td>\n",
       "      <td>0.702440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.723390</td>\n",
       "      <td>0.710238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-320\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-320/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-320/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-640\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-640/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-640/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-960\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-960/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-960/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1280\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1280/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1280/pytorch_model.bin\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to adapter_checkPoints/gender/checkpoint-1600\n",
      "Configuration saved in adapter_checkPoints/gender/checkpoint-1600/config.json\n",
      "Model weights saved in adapter_checkPoints/gender/checkpoint-1600/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from adapter_checkPoints/gender/checkpoint-960 (score: 0.7133333333333334).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 64\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.8028571428571428: 100%|█████████████████████████████████████████████████████| 2800/2800 [00:17<00:00, 163.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with 512 authors per label:  {'accuracy': [0.8098571428571427, 0.007987873462253879], 'f1-score': [0.8000596537338568, 0.013255013701296225]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoAdapterModel\n",
    "from transformers import TrainingArguments, Trainer, AdapterTrainer, EarlyStoppingCallback\n",
    "from TestingPAN17 import compute_accuracy, compute_test_metrics\n",
    "\n",
    "num_v           = len(baseTest.variety_dict)\n",
    "num_labels_dict = {\"gender\": 2, \"variety\": num_v,}\n",
    "\n",
    "FewShot_Results = {}\n",
    "\n",
    "for num in _NUM_AUTHORS_:\n",
    "    # SHOW CURRENT PORTION\n",
    "    print(\"Working with \" + str(num) + \" authors per label ... \")\n",
    "    \n",
    "    dataset_dict = {}\n",
    "    models = {}\n",
    "    \n",
    "    for task_name in tasks:\n",
    "        \n",
    "        acc = []\n",
    "        f1s = []\n",
    "        \n",
    "        for val_idx in range(_K_FOLD_CV_):\n",
    "            print(\"Train,Val split number \" + str(val_idx + 1) + \" of \" + str(_K_FOLD_CV_))\n",
    "        \n",
    "            # INITIALIZE MODEL-----------------------------------------\n",
    "            models[task_name] = AutoModelForSequenceClassification.from_pretrained(\n",
    "                _PRETRAINED_LM_,\n",
    "                num_labels = num_labels_dict[task_name]\n",
    "            )\n",
    "            \n",
    "            # GENERATES DATASET WITH CURRENT PORTION ----------------------\n",
    "            \n",
    "            data_train, data_val = baseTrain.cross_val(k = _K_FOLD_CV_, val_idx = val_idx, num_authors = num)\n",
    "            Train = DatasetCrossVal(Base_Data = data_train, label = 'gender')\n",
    "            Val   = DatasetCrossVal(Base_Data = data_val  , label = 'gender')\n",
    "\n",
    "            # TRAIN ADAPTER--------------------------------------------\n",
    "\n",
    "            training_args = TrainingArguments(\n",
    "                learning_rate               = _LEARNING_RATE_,\n",
    "                num_train_epochs            = _EPOCHS_[task_name],\n",
    "                per_device_train_batch_size = _BATCH_SIZE_,\n",
    "                per_device_eval_batch_size  = _BATCH_SIZE_,\n",
    "                output_dir                  = _OUTPUT_DIR_ + '/' + task_name,\n",
    "                save_total_limit            = 5,\n",
    "                overwrite_output_dir        = True,\n",
    "                remove_unused_columns       = False,\n",
    "                evaluation_strategy         = 'epoch',\n",
    "                logging_strategy            = 'epoch',\n",
    "                save_strategy               = 'epoch',\n",
    "                metric_for_best_model       = 'eval_acc',\n",
    "                load_best_model_at_end      = True,\n",
    "            )\n",
    "\n",
    "            trainer = Trainer(\n",
    "                model           = models[task_name],\n",
    "                args            = training_args,\n",
    "                train_dataset   = Train,\n",
    "                eval_dataset    = Val,\n",
    "                compute_metrics = compute_accuracy,\n",
    "                callbacks       = [EarlyStoppingCallback(early_stopping_patience = 2, early_stopping_threshold = 0.0001)]\n",
    "            )\n",
    "            trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "            trainer.train()\n",
    "\n",
    "            # TEST MODEL ------------------------------------\n",
    "\n",
    "            results = trainer.predict(Test)\n",
    "            metrics = compute_test_metrics(baseTest, results.predictions, 'gender')\n",
    "            \n",
    "            acc.append(metrics['accuracy'])\n",
    "            f1s.append(metrics['f1-score'])\n",
    "           \n",
    "        acc = np.array(acc)\n",
    "        f1s = np.array(f1s)\n",
    "        \n",
    "        FewShot_Results[num] = {'accuracy': [acc.mean(), acc.std()], 'f1-score': [f1s.mean(), f1s.std()]}\n",
    "        print(\"Results with \" + str(num) + \" authors per label: \", FewShot_Results[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2180ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8: {'accuracy': [0.6463571428571429, 0.038807741706089145],\n",
       "  'f1-score': [0.675614778161259, 0.05229066275650628]},\n",
       " 16: {'accuracy': [0.6705, 0.01659634825828275],\n",
       "  'f1-score': [0.6892745171156573, 0.02207377870412438]},\n",
       " 32: {'accuracy': [0.679142857142857, 0.008611809641772302],\n",
       "  'f1-score': [0.6896027128017307, 0.006236413401855358]},\n",
       " 48: {'accuracy': [0.6794285714285715, 0.013722095990312377],\n",
       "  'f1-score': [0.6935243485276106, 0.025162403949095986]},\n",
       " 64: {'accuracy': [0.6882857142857143, 0.03321113656345916],\n",
       "  'f1-score': [0.6787820866714995, 0.048350822584644085]},\n",
       " 128: {'accuracy': [0.7292142857142858, 0.007379646581881108],\n",
       "  'f1-score': [0.7342845402829002, 0.013718943601915282]},\n",
       " 256: {'accuracy': [0.7807857142857142, 0.012316821084705786],\n",
       "  'f1-score': [0.775970202717284, 0.014919221734025953]},\n",
       " 512: {'accuracy': [0.8098571428571427, 0.007987873462253879],\n",
       "  'f1-score': [0.8000596537338568, 0.013255013701296225]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FewShot_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a0128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
